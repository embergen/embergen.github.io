[{"content":"Ensemble Learning has the flexibility of a CART but without the tendency to memorize noise.\nThe process:\nTrain different models on same data Make predictions with each model Use meta-model to aggregate the predictions and create a final prediction. This final prediction should be more robutst than the individual models. For best results, the models should be skillful in different ways.\nOne example of ensemble learning for classification tasks is sklearn\u0026rsquo;s Voting Classifier. There are two types of voting classifier:\nHard voting: The predicted class is the one with the highest majority of votes among the models. (e.g. Two out of three models predicted an output of 1, so the final prediction is 1.) Soft voting: The model takes the averages of the probabilities from the models and uses that average to determine the final prediction. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # Run the imports for metrics, splitting, models, and voting classifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier as KNN from sklearn.ensemble import VotingClassifier # Set the seed SEED = 99 # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED) # Instantiate the various models lr = LogisticRegression(random_state=SEED) knn = KNN() dt = DecisionTreeClassifier(random_state=SEED) # Create a classifier list with tuples for each one\u0026#39;s name classifiers = [(\u0026#34;Logistic Regression\u0026#34;, lr), (\u0026#34;K Nearest Neighbors\u0026#34;, knn), (\u0026#34;Classification Tree\u0026#34;, dt)] # Use a for loop to fit, predict, and print accuracy for each model (for comparison) for clf_name, clf in classifiers: clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\u0026#34;{:s} : {:.3f}\u0026#34;.format(clf_name, accuracy)) # Instantiate the voting classifier vc = VotingClassifier(estimators=classifiers) # Fit vc and predict vc.fit(X_train, y_train) y_pred = vc.predict() # Evaluate accuracy for vc print(\u0026#34;Voting Classifier: {.3f}\u0026#34;.format(accuracy_score(y_test, y_pred))) ","date":"2025-02-18T00:00:00Z","permalink":"https://embergen.github.io/p/ensemble-learning/","title":"Ensemble Learning"},{"content":"Remember, we want to avoid overfitting and underfitting. In overfitting, the model fits the noise in the training set and will not handle new data well. In underfitting, the model is not flexible enough to capture the relationship between variables and thus will also not be helpful in making predictions from new data.\nA generalization error is a measure of how well a model generalizes to new data. It is based on bias, variance, and irreducible error. We want the lowest possible generalization error, finding a balance between bias and variance.\nBias: The errors a model makes due to its assumptions. Can lead to underfitting if the model fails to capture important patterns. Example: If a simple linear regression model is trained on complex non-linear data, it will likely have high bias. To reduce bias: Use more complex models, add features, or improve feature engineering. Variance: How sensitive the model is to fluctuations in the data. Can lead to overfitting if the model is trained to the noise rather than the underlying patterns. Example: A complex model with a lot of parameters fit to a small dataset may have high variance. To reduce variance: Use regularization techniques, cross-validation, reduce model complexity, or gather more data. Irreducible error: Noise. Errors that cannot be reduced due to choosing a \u0026ldquo;better\u0026rdquo; model. As bias increases, variance decreases, and vice versa. This is the bias-variance tradeoff.\nSo, how do we practically deal with this? How do we estimate the generalization error?\nWe use cross-validation (K-Fold CV, Hold-Out CV) and compute the mean of the errors, then compare that to the training set errors.\nIf CV error \u0026gt; training error: the model suffers from high variance (overfitting). If CV error is similar to training error but greater than desired error: the model suffers from high bias (underfitting). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Run the imports from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import train_test_split, cross_val_score from sklearn.metrics import mean_squared_error as MSE # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=99) # Instantiate the decision tree regressor dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=99) # Evaluate the MSE/RMSE from 10-fold CV MSE_CV = cross_val_score(dt, X_train, y_train, cv=10, scoring=\u0026#39;neg_mean_squared_error\u0026#39;, n_jobs=-1) RMSE_CV = (MSE_CV.mean())**(1/2) # Fit the model dt.fit(X_train, y_train) # Create predictions from both train and test sets y_predict_train = dt.predict(X_train) y_predict_test = dt.predict(X_test) # Evaluate the MSE/RMSE from training and test sets (Recall: RMSE is easier to interpet) MSE_train = MSE(y_train, y_predict_train) MSE_test = MSE(y_test, y_predict_test) RMSE_train = MSE_train ** (1/2) RMSE_test = MSE_test ** (1/2) # Print the MSE for the three outputs print(\u0026#34;CV MSE: {:.2f}\u0026#34;.format(MSE_CV.mean())) print(\u0026#34;Train MSE: {:.2f}\u0026#34;.format(MSE_train)) print(\u0026#34;Test MSE: {:.2f}\u0026#34;.format(MSE_test)) # Or, print the RMSE: print(\u0026#34;CV RMSE: {:.2f}\u0026#34;.format(RMSE_CV.mean())) print(\u0026#34;Train RMSE: {:.2f}\u0026#34;.format(RMSE_train)) print(\u0026#34;Test RMSE: {:.2f}\u0026#34;.format(RMSE_test)) ","date":"2025-02-14T00:00:00Z","permalink":"https://embergen.github.io/p/generalization-error-and-the-bias-variance-tradeoff/","title":"Generalization Error and the Bias-Variance Tradeoff"},{"content":"In the last couple of practice sessions, I was looking at regression for classification. How do they work for a regression problem, when the target variable is continuous?\nIn a regression tree, the impurity is measuring by the MSE of the targets in a node - how spread out/mixed the target values are. The goal for each split is then to minimize this impurity. (A lower MSE = a more pure/homegenous node with less variance in the target values.)\nWhy use a regression tree over a typical linear regression model? One benefit is that decision trees are more flexible - they can better capture non-linear relationships between continuous variables and the target.\nWe can use scikit-learn\u0026rsquo;s DecisionTreeRegressor() to create a model for regression. The \u0026ldquo;min_samples_leaf\u0026rdquo; parameter below is set to 0.1, meaning that each leaf has to include at least 10% of the training data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Run the imports from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import train_test_splitl from sklearn.metrics import mean_squared_error as MSE # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99) # Instantiate the model dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=99) # Fit and predict dt.fit(X_train, y_train) y_pred = dt.predict(X_test) # Obtain MSE mse_dt = MSE(y_test, y_pred) # Use MSE to get RMSE (power of 1/2 is equivalent of square root) rmse_dt = mse_dt**(1/2) # Print the score to 2 decimal places print(\u0026#34;Test set RMSE of dt: {:.2f}\u0026#34;.format(rmse_dt)) ","date":"2025-02-12T00:00:00Z","permalink":"https://embergen.github.io/p/decision-trees-for-regression/","title":"Decision Trees for Regression"},{"content":"Important vocab:\nDecision Tree: A data structure with a hierarchy of nodes Node: A question/prediction point in the tree Root Node: The initial node. No parents, two children. Internal Node: A node with one parent and two children. Leaf: A node with one parent and NO children. Where the prediction is finally made. Each with children asks a question involving one feature (f) and a split point (sp). How does it know which feature and which split point to pick?\nThe model tries to maximize the information gain from each split. I found this link helfpul in getting a better idea of what this means. Basically, you can calculate the entropy (impurity/randomness) the parent and child nodes. Ideally, we want the data in each split group to belong to the same class - the one we are trying to predict.\nThe difference between the entropy of the parent and the weighted average of the entropy of the child nodes is the information gain. The feature with the highest potential information gain at each node is the one that should be used to split the data.\nIf the information gain for a node is 0, then that node is a leaf. A node is also a leaf if it is at the maximum depth declared for the decision tree.\nThe Gini Index is another way of measuring the purity of the data group.\nA lower index indicates higher purity/lower diversity A higher index indicates lower purity/higher diversity So if you set the criterion to \u0026ldquo;gini\u0026rdquo; when building your model, the model will try different splits and compare the Gini Indexes to choose the ideal split question for that node. The Gini Index is slightly faster than entropy and is the default criterion for DecisionTreeClassifier().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Run the imports from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=99) # Instantiate the model with Gini Index (alternative: criterion=\u0026#39;entropy\u0026#39;) dt = DecisionTreeClassifier(criterion=\u0026#39;gini\u0026#39;, random_state=99) # Fit and predict dt.fit(X_train, y_train) y_pred = dt.predict(X_test) # Evaluate accuracy accuracy_score(y_test, y_pred) ","date":"2025-02-11T00:00:00Z","permalink":"https://embergen.github.io/p/decision-tree-learning/","title":"Decision Tree Learning"},{"content":"The last few entries have been a back-to-basics review of concepts I\u0026rsquo;ve already learned. I\u0026rsquo;m excited now to dive into something that I know of, but have not actually used a lot.\nCART stands for \u0026ldquo;Classification and Regression Trees.\u0026rdquo;\nAdvantages: Simple to understand and interpret Easy to use Flexible with non-linear dependences between features/target No need to standardize/normalize features beforehand Limitations: Classification trees can only produce orthoganal decision boundaries (perpendicular to axis) Sensitive to small variations in the training set High variance if trained without constraints (\u0026ndash;\u0026gt; overfitting) What are classification trees? How do they work?\nThey use a sequence of if-else statements about features to infer labels. Each statement has one feature and one split. They can work with non-linear relationships between features and labels. You do not have to standardize the data. The maximum depth is the maximum branches between the top and an extreme end.\nA classification tree creates rectangular decision regions (regions where instances are assinged a class label), separated by decision boundaries. Note the difference between how logistic regression and decision trees divide the instances:\nSetting up a basic decision tree in python:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Run the imports from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=99) # Instantiate the model dt= DecisionTreeClassifier(max_depth=2, random_state=99) # Fit the model to the training data dt.fit(X_train, y_train) # Predict from the test set y_pred = dt.predict(X_test) # Evaluate the accuracy accuracy_score(y_test, y_pred) You can plot decision tree regions like in the image above. The code below assumes you already have a \u0026ldquo;logreg\u0026rdquo; model and a \u0026ldquo;dt\u0026rdquo; model.\n1 2 3 4 5 # Make a list of your models models = [logreg, dt] # Show the decision regions plot_labeled_decision_regions(X_test, y_test, models) ","date":"2025-02-10T00:00:00Z","permalink":"https://embergen.github.io/p/decision-tree-classification/","title":"Decision Tree Classification"},{"content":"How do you know which model to use for Machine Learning? There are a few factors that can affect the decision:\nSize of the dataset If there are fewer features, the training time is faster Some models won\u0026rsquo;t work well with smaller datasets (e.g. ANN) Interpretability The results of some models (e.g. linear regression) are easier to explain to people outside of data science Flexibility A flexible model may make fewer assumptions about the data (e.g. KNN not assuming linear relationship) which can boost accuracy We use different metrics to evaluate different models:\nRegression: RMSE, R-squared Classification: Accuracy/precision/recall/F1 score, confusion matrix, ROC AUC Using evaluation metrics, you can simply train multiple models and compare the results. Models that typically require scaling should still undergo scaling before running the evaluations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # Run the imports for plotting, scaling, and model creation import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.model_selection import cross_val_score, KFold, train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier # Create feature and target arrays X = df.drop(\u0026#34;target\u0026#34;, axis=1).values y = df[\u0026#34;target\u0026#34;].values # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99) # Scale the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Create a dictionary of model names models = {\u0026#34;Logistic Regression\u0026#34;: LogisticRegression(), \u0026#34;KNN\u0026#34; = KNeighborsClassifier(), \u0026#34;Decision Tree\u0026#34;: DecisionTreeClassifier()} # Create an empty list to store the results results = [] # Loop through the models with default scoring (accuracy) and append results for each to list for model in models.values(): kf = KFold(n_splits=6, random_state=99, shuffle+=True) cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf) results.append(cv_results) # Create a boxplot of the results plt.boxplot(results, labels=models.keys()) plt.show() # To evaluate on the test set for name, model in models.items(): # Fit the model model.fit(X_train_scaled, y_train) # Calculate accuracy and print the results test_score = model.score(X_test_scaled, y_test) print(\u0026#34;{} Test Set Accuracy: {}\u0026#34;.format(name, test_score)) ","date":"2025-02-04T00:00:00Z","permalink":"https://embergen.github.io/p/choosing-a-model/","title":"Choosing a Model"},{"content":"Why is it important to scale our data? Different numeric variables have have very different ranges. One might be represented only by values from 0 to 1, while another might be in the millions (or millionths).\nMany ML models (e.g. KNN, linear/ridge/lasso, log reg, ANN) are influenced by distance, so a feature with a large scale would impact the model more than a feature with a small scale. So, we standardize our data so that our features are represented by similar scales. (There are other methods as well.)\nStandardization What does standardization do?\nSubtract the mean Divide by the variance Result? All features are centered around zero and have a variance of one.\nHow-to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Import StandardScaler from sklearn.preprocessing import StandardScaler # Create X and y and split into train/test sets X = df.drop(\u0026#34;color\u0026#34;, axis=1\u0026#34;).values y = df[\u0026#34;color\u0026#34;].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 99) # Instantiate the scaler scaler = StandardScaler() # Fit transform the training features X_train_scaled = scaler.fit_transform(X_train) # Transform the test features X_test_scaled = scaler.transform(X_test) # Optionally, vertify the changes by comparing the old and new mean/std print(np.mean(X), np.std(X)) print(np.mean(X_train_scaled), np.std(X_train_scaled)) Scalers also work well in pipelines, like I used in yesterday\u0026rsquo;s notes on handling missing data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Create a list of tuples with step names and our transformer/model steps = [(\u0026#34;scaler\u0026#34;, StandardScaler()), (\u0026#34;knn\u0026#34;, KNeighborsClassifier(n_neighbors=6))] # Create the pipeline and feed it the steps pipeline = Pipeline(steps) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 99) # Fit the pipeline to the training set knn_scaled = pipeline.fit(X_train, y_train) # Predict from the test set y_pred = knn_scaled.predict(X_test) # Check the model\u0026#39;s accuracy print(knn_scaled.score(X_test, y_test)) Let\u0026rsquo;s add another layer. In this pipeline, we will also include a Grid Search to see which value for n_neighbors performs best.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Import Grid Search from sklearn.model_selection import GridSearchCV # Create steps and pipeline steps = [(\u0026#34;scaler\u0026#34;, StandardScaler()), (\u0026#34;knn\u0026#34;, KNeighborsClassifier())] pipeline = Pipeline(steps) # Create an array of values to try for n_neighbors (range of integers from 1 to 49) params = {\u0026#34;knn__n_neighbors\u0026#34;: np.arange(1, 50)} # Split data into train/test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99) # Perform a grid search cv = GridSearchCV(pipeline, param_grid = params) # Fit the grid search object to the training data cv.fit(X_train, y_train) # Predict from the test set y_pred = cv.predict(X_test) # Show the best score and best value for n_neighbors from the grid search CV print(cv.best_score_, \u0026#34;\\n\u0026#34;, cv.best_params_) ","date":"2025-01-30T00:00:00Z","permalink":"https://embergen.github.io/p/normalization/","title":"Normalization"},{"content":"A missing value referes to a cell that is blank or null in some way. It might be represented as NaN, null, NA, or given a code.\nMissing Completely at Random (MCAR): The missing value is completely independent of other variables and fully random. Missing at Random (MAR): The missing value is related to other variables. Missing Not at Random (MNAR): The missing value is related to the data point itself. The missing data needs to be dealt with. We can check how many missing values there are in each feature. The following will return a table with the feature nammes on the left and the sum of missing values on the right.\n1 2 # Show missing values per column in ascending order of missing values print(df.isna().sum().sort_values()) So, what are the options for handling missing values?\nDropping Observations We can use pandas\u0026rsquo; dropna() function to remove missing observations. We can specify a subset of features that we want to check for null values. This removes all rows with missing values in those columns. As a result, you wouldn\u0026rsquo;t want to use this strategy indiscriminately.\n1 df = df.dropna(subset=[\u0026#34;feature1\u0026#34;, \u0026#34;feature2\u0026#34;, \u0026#34;feature3\u0026#34;]) Imputing Values We can also impute the missing values, filling them with logical guesses bases on knowledge about the data/subject.\nNumeric features: It\u0026rsquo;s common to use the mean of the non-missing values in the feature, but you can also use the median etc. Category features: It\u0026rsquo;s common to use the mode of the feature Note: You must split the data before imputing values. Otherwise, you will end up with data leakage. Data leakage in machine learning is when a model uses information during training that it shouldn\u0026rsquo;t have access to. The reason this specific instance would cause data leaking is that the calculated mean/mode of a feature for the entire dataset may be different than the mean/mode of the training set.\nWe can conduct imputation using sklearn:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # Import SimpleImputer from sklearn.impute import SimpleImputer # Since categorical and numeric features will be treated differently, we separate them X_cat = df[\u0026#34;color\u0026#34;].values.reshape(-1,1) X_num = df.drop([\u0026#34;color\u0026#34;, \u0026#34;target\u0026#34;], axis=1).values y = df[\u0026#34;target\u0026#34;].values # Split the categorical/numeric into training and test sets X_train_cat, X_test_cat, y_train, y_test = train_test_split(X_cat, y, test_size=0.2, random_state=99) X_train_num, X_test_num, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=99) # Instantiate the imputer for categorical features using mode imp_cat = SimpleImputer(strategy=\u0026#34;most_frequent\u0026#34;) # Call fit_transform on the training categorical features and transform on the test categorical features X_train_cat = imp_cat.fit_transform(X_train_cat) X_test_cat = imp_cat.transform(X_test_cat) # Instantiate a new imputer for the numeric features (default = mean) imp_num = SimpleImputer() # Call fit transform on the training numeric features and transform on the test numeric features X_train_num = imp_num.fit_transform(X_train_num) X_test_num = imp_num.transform(X_test_num) # Combine the categorical and numeric training features into X_train and X_test X_train = np.append(X_train_num, X_train_cat, axis=1) X_test = np.append(X_test_num, X_test_cat, axis=1) Pipelines We can simplify this process with a pipeline. A pipeline will run transformations and build a model using a single workflow.\nIn the example below, we will do binary classification to predict if an observation is purple or not.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Import the pipeline from sklearn.pipeline import Pipeline # Make color a binary column where all purples are set to 1 and other values are set to 0 df[\u0026#34;color\u0026#34;] = np.where(df[\u0026#34;genre\u0026#34;] == \u0026#34;purple\u0026#34;, 1, 0) # Separate the features and target variable into X and y X = df.drop(\u0026#34;color\u0026#34;, axis=1).values y = df[\u0026#34;color\u0026#34;].values # Create a list of tuples telling the pipeline what to do # Each tuple has a step name as a string and a transformer/model instantiation # Each step except the last needs to be a transformer steps = [(\u0026#34;imputation\u0026#34;, SimpleImputer()), (\u0026#34;log_regression\u0026#34;, LogisticRegression())] # Instantiate the pipeline, passing the list of steps pipeline = Pipeline(steps) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99) # Fit the pipeline to the training data pipeline.fit(X_train, y_train) # Compute accuracy pipeline.score(X_test, y_test) ","date":"2025-01-29T00:00:00Z","permalink":"https://embergen.github.io/p/missing-data-and-pipelines/","title":"Missing Data and Pipelines"},{"content":"In scikit-learn, our data needs to be numeric and have no missing values. However, data rarely starts out that way. Preprocessing is when we prepare and transform raw data so that it is suitable for our ML models.\nCategorical Features Categorical features (e.g. color, country, sex) can be converted into numeric binary values by using a dummy variable for each category. For example, let\u0026rsquo;s say there was a category \u0026ldquo;color\u0026rdquo; with three possible values: green, purple, and yellow. Instead of having one column called \u0026ldquo;color,\u0026rdquo; we could have one column named after each color. Then, the value for each observation would be either 0 or 1 for these columns. If an observation was green, it would have a 1 under green and a 0 under purple and yellow.\nHowever, we can actually accomplish this with one less column. If we have a column for \u0026ldquo;green\u0026rdquo; and \u0026ldquo;purple\u0026rdquo; but not \u0026ldquo;yellow\u0026rdquo;, we can still express that something is yellow by giving it a 0 in \u0026ldquo;green\u0026rdquo; and \u0026ldquo;purple.\u0026rdquo; Since there are only three possible colors, if it is not green or purple it must be yellow.\nThere are two common ways to create dummy variables in python: pandas\u0026rsquo; get_dummies() and scikit-learn\u0026rsquo;s OneHotEncoder().\nget_dummies() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Import pandas import pandas as pd # Read in the df my_df = pd.read_csv(\u0026#39;mydata.csv\u0026#39;) # Create dummy variables for our categorical feature. # drop_first will drop one of the dummmies so that it is represented by all 0s in the other dummies my_dummies = pd.get_dummies(my_df[\u0026#34;color\u0026#34;], drop_first=True) # Check the results print(my_dummies.head()) # Combine the dummies with your original df and remove the original categorical feature my_dummies = pd.concat([my_df, my_dummies], axis=1) my_dummies = my_dummies.drop(\u0026#34;color\u0026#34;, axis=1) Sidenote: If there is only one categorical feature, you can just pass the whole df into get_dummies() instead of specifying the columm. If you do so, the dummy columns will be prefixed with the original column name (e.g. color_green) and the original categorical feature will be dropped automatically.\nEither way, the df with the dummy variables can now be used for ML:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Run the imports from sklearn.model_selection import cross_val_score, KFold from sklearn.linear_model import LinearRegression # Separate the target from the features X = my_dummies.drop(\u0026#34;target\u0026#34;, axis=1).values y = my_dummies[\u0026#34;target\u0026#34;].values # Create the training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99) # Create a KFold object kf = KFold(n_splits = 5, shuffle = True, random_state=99) # Instantiate the model linreg = LinearRegression() # Call cross_val_score # sklearn\u0026#39;s cross-val metrics presume higher scores are better, so we use negative MSE linreg_cv = cross_val_score(linreg, X_train, y_train, cv=kf, scoring=\u0026#34;neg_mean_squared_error\u0026#34;) # Calculate training RMSE from negative MSE print(np.sqrt(-linreg_cv)) ","date":"2025-01-28T00:00:00Z","permalink":"https://embergen.github.io/p/preprocessing-data-for-ml/","title":"Preprocessing Data for ML"},{"content":"Different models have different hyperparameters that we can tune to increase model performance. Hyperparameters are parameters that we specify before fitting the model to the data.\nRidge/lasso regression: Alpha KNN: n_neighbors (Sidenote: Hyperparameters are variables that the user specifies. Parameters are variables such as coefficients that are just part of the model.)\nHyperparameter tuning is a process for choosing the optimal hyperparameters. In this process, you try various hyperparamter values, fit them separately, and judge their performance to find the best values. When doing so, it is important to use cross-validation (CV) so we don\u0026rsquo;t end up with overfitting.\nGrid search CV and random search CV are two potential methods for using CV to tune our hyperparameters.\nGrid Search CV In grid search CV, you choose which hyperparameter values to try and which metric(s) to evaluate them by. This returns a grid of metric results.\nFor example, the table below shows the results for n_neighbors 2, 5, 8, and 11 with euclidean and manhattan metrics. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Import GridSearchCV from sklearn.model_selection import GridSearchCV # Instantiate KFold kf = KFold(n_splits=5, shuffle=True, random_state=99) # Create the parameter grid dictionary we want to use for tuning # Alphas: in this case, a range of twenty evenly-spaced values from 0.00001 to 1 # Solver: the metrics (defaults to R-squared) param_grid = {\u0026#34;alpha\u0026#34;: np.linspace(0.00001, 1, 20), \u0026#34;solver\u0026#34;: [\u0026#34;sag\u0026#34;, \u0026#34;lsqr\u0026#34;]} # Instantiate the model (ridge, in this example) ridge = Ridge() # Create a grid search object, passing the model and parameter grid ridge_cv = GridSearchCV(ridge, param_grid, cv=kf) # Fit the grid search object to the training data ridge_cv.git(X_train, y_train) # Print the model\u0026#39;s best parameters and best score # In this case, it will return the best alpha value, the best solver, and the mean CV score for that fold print(ridge_cv.best_params_, ridge_cv.best_score_) Grid search CV can be problematic because it doesn\u0026rsquo;t scale well. The time and resources required is a product of the number of folds, the number of hyperparameters, and the number of values for each hyperparameter.\ne.g. 3 folds, 1 hyperparameter, 10 values = 30 fits 10 folds, 3 hyperparamters, 30 values = 900 fits Random Search CV In Random Search CV, you still feed the model specific values, but instead of running thorugh every possible combination of values, it will randomly select combinations to test. As a result, random search is less resource demanding than grid search. The process in scikit-learn is very similar to the one above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Import Random Search CV from sklearn.model_selection import RandomizedSearchCV # Instantiate KFold kf = KFold(n_splits=5, shuffle=True, random_state=99) # Set up the parameter grid dictionary param_grid = {\u0026#34;alpha\u0026#34;: np.linspace(0.00001, 1, 20), \u0026#34;solver\u0026#34;:[\u0026#34;sag\u0026#34;, \u0026#34;lsqr\u0026#34;]} # Instantiate the model ridge = Ridge() # Create the grid search object, this time with the optional n_iter # n_iter determines how many values are tested per hyperparameter ridge_cv = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2) # Fit the model ridge_cv.fit(X_train, y_train) # Print the model\u0026#39;s best parameters and best score print(ridge_cv.best_params_, ridge_cv.best_score_) ","date":"2025-01-27T00:00:00Z","permalink":"https://embergen.github.io/p/model-hyperparameters/","title":"Model Hyperparameters"},{"content":"Logistic regression a type of regression used in supervised machine learning for binary classification problems. It used features to output the probability (p) on whether or not an observation belongs to a binary class.\nIf p \u0026gt; 0.5, the data is labeled as a member of that class (1). If p \u0026lt; 0.5, the data is NOT labeled as a member (0). How to perform logistic regression with scikit-learn:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Import the library from sklearn.linear_model import LogisticRegression # Instantiate the classifier model logreg = LogisticRegression() # Split the data X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 99) # Fit the model logreg.fit(X_train, y_train) # Predict from the test set y_pred = logreg.predict(X_test) We can also get the exact probability for each instance using the predict_proba method, which returns a 2D array of probilities for 0 (left column) and 1 (right column).\n1 2 # Apply predict_proba to the right column (index 1) y_pred_probs = logreg.predict_proba(X_test)[:, 1] The default threshold for p is 0.5, but you can adjust it. An ROC curve will show how different thresholds affect the true positive and false positive rates. As I talked about in yesterday\u0026rsquo;s post about the confusion matrix, the nature of your study will determine whether you care more about false positives (false alarms) or false negatives (missed true positives).\nWe can plot an ROC curve in Python using the y_pred_probs from the previous code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Import the library from sklearn.metrics import roc_curve # Call the roc_curve function. # fpr = false positive rate, tpr = true positive rate fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs) # Plot the line plt.plot([0, 1], [0, 1], \u0026#39;k--\u0026#39;) plt.plot(fpr, tpr) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Logistic Regression ROC Curve\u0026#39;) plt.show() The dotted line in the center represents a model that just randomly guesses. So what does this ROC model actually tell us?\nWe calculate the area under the ROC curve (AUC). Scores range from 0 to 1 with 1 being the best. If the AUC is 0.5, it means the model is no better than a random guess. When the ROC curve is above the dotted line, it means the model is better than just randomly guessing. 1 2 3 4 5 # Run the imports from sklearn.metrics import roc_auc_score # Call the auc score and print the results print(roc_auc_score(y_test, y_pred_probs)) ","date":"2025-01-23T00:00:00Z","permalink":"https://embergen.github.io/p/logistic-regression-and-roc-curves/","title":"Logistic Regression and ROC Curves"},{"content":"Metrics for Classification Models The word \u0026ldquo;acccuracy\u0026rdquo; is often used broadly to cover how close to the truth something is, but in data science the meaning is more specific. There are also metrics other than accuracy that will judge how correct a model is, and the one you choose depends on the purpose of your study. It might be preferable to have a false positive, or it might be more preferable to avoid a false negative, for example.\nNegatives/Positives in Classification There are four potential outcomes for your predictions when performing binary classification:\nTrue Positive (TP): You correctly labeled a positive data point as positive. False Positive (FP): You incorrectly labeled a negative data point as positive. True Negative (TN): You correctly labeled a negative data point as negative. False Negative (FN): You incorrectly labeled a positive data point as negative. As I mentioned, you will have different concerns based on the purpose of your analysis. For example, I did a machine learning assignment in school where I was using features to predict which students might need additional academic support. In my case, I was not overly concerned with false positives - if the school started academic intervention for a student who ended up not needing it, it would not be harmful for the student and the staff leading the support group would soon notice.\nIn other cases, you might be more concerned about false positives. For example, a spam detector that flags important emails as spam would not be useful to a client.\nA confusion matrix is a two-by-two grid which displays all of the values for TP, TN, FP, and FN.\nSource\nMetrics These four classification outcomes lead to four metrics for assessing a classification model:\nAccuracy: Out of all predictions, how many were correct? Formula: (TP + TN) / (TP + TN + FP + FN) Precision: Out of all the times the model predicted a positive, how often was that positive correct? Formula: TP / (TP + FP) High precision = lower FP rate Recall: Out of all the data points that should have been predicted as positive, how many were accurately predicted? Formula: TP / (TP + FN) High recall = lower FN rate F1 Score: The harmonic mean of precision and recall, giving them equal weight. Useful when you have imbalanced data or want to account for both FP and FN. Formula: 2 ((Precision * Recall) / (Precision + Recall)) Relying on the wrong metric can lead to a false understanding of how well your model is performing. For example, a model might have a high accuracy score, but this might simply be the result of imbalanced data.\nLet\u0026rsquo;s say you are trying to classify if something is a fish or a cow. If your data is 99% fish, and your model predicts that every sincle datapoint is a fish, then the accuracy score would be 99%. It sounds good, but it does not actually mean your model is good at differentiating between fish and cows.\nPython Applications 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Import the libraries from sklearn.metrics import classification_report, confusion_matrix # Instantiate the classifier knn = KNeighborsClassifier(n_neighbors = 7) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 99) # Fit the model with the training data knn.fit(X_train, y_train) # Predict the labels for the test set y_pred = knn.predict(X_test) # Generate a confusion matrix from the predicted labels and test labels print(confusion_matrix(y_test, y_pred)) # Generate metrics for the predicted and test labels print(classification_report(y_test, y_pred) Sample results of confusion matrix and classification report:\n","date":"2025-01-22T00:00:00Z","permalink":"https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/","title":"Classification Metrics and the Confusion Matrix"},{"content":"Overfitting and Regularization Overfitting is when a machine learning model fits too closely to its training data. It might look like the model is doing a good job since it can predict so well from the training data, but it will likely give inaccurate predictions when fed novel data.\nIt can happen when there is not enough training data, the training data is too noisy (i.e. irrelevant info, errors, etc.), the model trains too long one one dataset, or the model complexity is overly high.\nFor example, in the visual below, the overfitted model predicts each point with complete accuracy, but this model would not handle new data well. The optimal model on the left has higher residuals, but it will generalize better to new data.\nSource: https://www.freecodecamp.org/news/what-is-overfitting-machine-learning/\nRegularization is a collection of techniques used to prevent overfitting. These techniques penalize the features that are less influential on the model\u0026rsquo;s predictions. Two common regularization techniques are ridge regression and lasso regression. Without getting into the mathematical details, the main difference between these two is that lasso will zero out the less useful features, while ridge will just reduce them.\nWhich one to use?\nRidge may be better if you want to retain all of your predictors, as lasso can zero out some predictors. This can also be helpful if you have some highly correlated predictors. Lasso may be better if you have redundant predictors with negligible influence on the target variable, as it will drop them. This can lead to a more interpretable model. Because lasso tends to shrink the coefficients of less important features to zero, it can be used to assess feature importance. Analytics Vidhya breaks down the differences in more detail.\nParameters:\nalpha: a parameter we choose (similar to \u0026lsquo;k\u0026rsquo; in KNN). Controls the model complexity. If alpha = 0, this is just OLS, which can lead to overfitting. If alpha is very high, it can lead to underfitting (and therefore less accurate predictions) Ridge Regression with Scikit-Learn: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Import the libraries from sklearn.linear_model import Ridge # Instantiate Ridge with various alpha scores alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0] ridge_scores = [] for alpha in alphas: ridge = Ridge(alpha=alpha) # Fit the data ridge.fit(X_train, y_train) # Obtain R2 score = ridge_score(X_test, y_test) ridge_scores.append(score) # Show the list of ridge scores print(ridge_scores) Lasso Regression: Finding Important Features: Note that since we are just using this to find important features, we don\u0026rsquo;t need to split into training and test sets - we can use the full dataset.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Import the libraries from sklearn.linear_model import Lasso # Save features, targets, and feature variable names X = df.drop(\u0026#34;target_variable\u0026#34;, axis=1).values y = df[\u0026#34;target_variable\u0026#34;].values names = df.drop(\u0026#34;target_variable\u0026#34;, axis=1).columns # Instantiate Lasso lasso = Lasso(alpha=0.1) # Fit the model to the data and extract coefficients lasso_coef = lasso.fit(X, y).coef_ # Plot the coefficients for each feature plt.bar(names, lasso_coef) plt.xticks(rotation=45) plt.show() This will produce a bar chart showing the significance of the features. It helps us identify important predictors and communicate that to others.\nSource: Data Camp\n","date":"2025-01-21T00:00:00Z","permalink":"https://embergen.github.io/p/regression-regularization/","title":"Regression: Regularization"},{"content":"The R2 value that is returned is affected by how the data randomly happened to be split. Cross-validation helps by splitting the data into sets of groups called \u0026ldquo;folds\u0026rdquo;. Let\u0026rsquo;s say we split the data into 10 folds (k=10). We use the first fold as the test set, fit the model on the other folds, make predictions, and then calculate the test metric (e.g. R2). Then, the preocess is repeated but with the other folds taking turn as the test set. In the end, we will have 10 values for our test metric and can calculate their mean/median etc.\nThe more folds, the more computationally expensive the cross-validation is.\nBasic setup with scikit-learn:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Import the libraries from sklearn.model_selection import cross_val_score, KFold from sklearn.linear_model import LinearRegression # Call KFold (default n_splits is 5. Shuffles the dataset before splitting into folds) kf = KFold(n_splits=10, shuffle=True, random_state=99) # Instantiate the model reg_model = LinearRegression() # Call cross_val_score, returning an array of CV scores. The default metric is R2 cv_results = cross_val_score(reg_model, X, y, cv=kf) # Print the array of CV scores print(cv_results) # Print the mean, standard deviation print(np.mean(cv_results), np.std(cv_results)) # Print the 95% confidence interval print(np.quantile(cv_results, [0.025, 0.975])) ","date":"2025-01-17T00:00:00Z","permalink":"https://embergen.github.io/p/regression-cross-validation/","title":"Regression: Cross-Validation"},{"content":"I decided to get a blog going for my data science notes. It\u0026rsquo;s an excuse to get more familiar with github, and it\u0026rsquo;s a different way to document what I\u0026rsquo;m working on without the pressure of making a presentable project for my portfolio.\nSo, without further ado:\nBasic Regression with Scikit-Learn In linear regression, we fit a line to our data and use that line to predict values. To check accuracy we use an error function (aka loss/cost function).\nresidual: vertical distance between a data point and the line Ordinary Least Squares (OLS): Minimize the residual sum of squares (RSS). R2: A measure that shows how much of the variance is explained by the variable, on a scale from 0 to 1. (How well does the data fit the model, aka goodnes of fit. For example, an R2 of 30% means that 30% of the variability is explained by the model. Generally, a low R2 is undesirable. At the same time, a high R2 does not necessarily mean that your model is good. What counts as a \u0026ldquo;good\u0026rdquo; R2 value also depends on the application. Mean Squared Error (MSE): The mean of the RSS. Measured in squared units of the target variable. Root Mean Squared Error (RMSE): The root of the MSE. Here\u0026rsquo;s a simple sample workflow of using linear regression with model assessments:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Run the imports from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error # Split the data into train/test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99) # Instantiate the model reg_model = LinearRegression() # Fit the model on the training data reg_model.fit(X_train, y_train) # Predict y based on the test set y_pred = reg_model.predict(X_test) # Compute R2 reg_model.score(X_test, y_test) # Commpute RMSE mean_squared_error(y_test, y_pred, squared=False) ","date":"2025-01-16T00:00:00Z","permalink":"https://embergen.github.io/p/regression-basics/","title":"Regression Basics"}]