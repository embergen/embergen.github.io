<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Classification on Esther's Learning Journal</title><link>https://embergen.github.io/tags/classification/</link><description>Recent content in Classification on Esther's Learning Journal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 11 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://embergen.github.io/tags/classification/index.xml" rel="self" type="application/rss+xml"/><item><title>Decision Tree Learning</title><link>https://embergen.github.io/p/decision-tree-learning/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://embergen.github.io/p/decision-tree-learning/</guid><description>&lt;p>Important vocab:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Decision Tree&lt;/strong>: A data structure with a hierarchy of nodes&lt;/li>
&lt;li>&lt;strong>Node&lt;/strong>: A question/prediction point in the tree
&lt;ul>
&lt;li>&lt;strong>Root Node&lt;/strong>: The initial node. No parents, two children.&lt;/li>
&lt;li>&lt;strong>Internal Node&lt;/strong>: A node with one parent and two children.&lt;/li>
&lt;li>&lt;strong>Leaf&lt;/strong>: A node with one parent and NO children. Where the prediction is finally made.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Each with children asks a question involving one feature (f) and a split point (sp). How does it know which feature and which split point to pick?&lt;/p>
&lt;p>The model tries to maximize the &lt;strong>information gain&lt;/strong> from each split. I found &lt;a class="link" href="https://medium.com/@ompramod9921/decision-trees-6a3c05e9cb82#:~:text=Information%20gain%20is%20a%20measure,It%20specifies%20randomness%20in%20data." target="_blank" rel="noopener"
>this link&lt;/a> helfpul in getting a better idea of what this means. Basically, you can calculate the entropy (impurity/randomness) the parent and child nodes. Ideally, we want the data in each split group to belong to the same class - the one we are trying to predict.&lt;/p>
&lt;p>The difference between the entropy of the parent and the weighted average of the entropy of the child nodes is the information gain. The feature with the highest potential information gain at each node is the one that should be used to split the data.&lt;/p>
&lt;p>If the &lt;strong>information gain&lt;/strong> for a node is 0, then that node is a &lt;strong>leaf&lt;/strong>. A node is also a leaf if it is at the maximum depth declared for the decision tree.&lt;/p>
&lt;p>The &lt;strong>Gini Index&lt;/strong> is another way of measuring the purity of the data group.&lt;/p>
&lt;ul>
&lt;li>A lower index indicates higher purity/lower diversity&lt;/li>
&lt;li>A higher index indicates lower purity/higher diversity&lt;/li>
&lt;/ul>
&lt;p>So if you set the criterion to &amp;ldquo;gini&amp;rdquo; when building your model, the model will try different splits and compare the Gini Indexes to choose the ideal split question for that node. The Gini Index is slightly faster than entropy and is the default criterion for DecisionTreeClassifier().&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Run the imports&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.tree&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">DecisionTreeClassifier&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.model_selection&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">train_test_split&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.metrics&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">accuracy_score&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Split the data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">X_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_test&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">train_test_split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stratify&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">random_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">99&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Instantiate the model with Gini Index (alternative: criterion=&amp;#39;entropy&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">DecisionTreeClassifier&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">criterion&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;gini&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">random_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">99&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Fit and predict&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y_pred&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_test&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Evaluate accuracy&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">accuracy_score&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_pred&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Decision Tree Classification</title><link>https://embergen.github.io/p/decision-tree-classification/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://embergen.github.io/p/decision-tree-classification/</guid><description>&lt;p>The last few entries have been a back-to-basics review of concepts I&amp;rsquo;ve already learned. I&amp;rsquo;m excited now to dive into something that I know &lt;em>of&lt;/em>, but have not actually used a lot.&lt;/p>
&lt;p>&lt;strong>CART&lt;/strong> stands for &amp;ldquo;Classification and Regression Trees.&amp;rdquo;&lt;/p>
&lt;ul>
&lt;li>Advantages:
&lt;ul>
&lt;li>Simple to understand and interpret&lt;/li>
&lt;li>Easy to use&lt;/li>
&lt;li>Flexible with non-linear dependences between features/target&lt;/li>
&lt;li>No need to standardize/normalize features beforehand&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Limitations:
&lt;ul>
&lt;li>Classification trees can only produce orthoganal decision boundaries (perpendicular to axis)&lt;/li>
&lt;li>Sensitive to small variations in the training set&lt;/li>
&lt;li>High variance if trained without constraints (&amp;ndash;&amp;gt; overfitting)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>What are classification trees? How do they work?&lt;/p>
&lt;ul>
&lt;li>They use a sequence of if-else statements about features to infer labels. Each statement has one feature and one split.&lt;/li>
&lt;li>They can work with non-linear relationships between features and labels.&lt;/li>
&lt;li>You do not have to standardize the data.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://embergen.github.io/p/decision-tree-classification/decision_tree2.png"
width="600"
height="520"
srcset="https://embergen.github.io/p/decision-tree-classification/decision_tree2_hu_e8bdbeb8eb7d0d6d.png 480w, https://embergen.github.io/p/decision-tree-classification/decision_tree2_hu_9b4894e2398c8209.png 1024w"
loading="lazy"
alt="Visualization of a decision tree"
class="gallery-image"
data-flex-grow="115"
data-flex-basis="276px"
>&lt;/p>
&lt;p>The &lt;strong>maximum depth&lt;/strong> is the maximum branches between the top and an extreme end.&lt;/p>
&lt;p>A classification tree creates rectangular &lt;strong>decision regions&lt;/strong> (regions where instances are assinged a class label), separated by &lt;strong>decision boundaries&lt;/strong>. Note the difference between how logistic regression and decision trees divide the instances:&lt;/p>
&lt;p>&lt;img src="https://embergen.github.io/p/decision-tree-classification/decision_regions.png"
width="700"
height="271"
srcset="https://embergen.github.io/p/decision-tree-classification/decision_regions_hu_1811526eb7478b9.png 480w, https://embergen.github.io/p/decision-tree-classification/decision_regions_hu_c1a36c128e9edd86.png 1024w"
loading="lazy"
alt="Source: DataCamp"
class="gallery-image"
data-flex-grow="258"
data-flex-basis="619px"
>&lt;/p>
&lt;p>Setting up a basic decision tree in python:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Run the imports&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.tree&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">DecisionTreeClassifier&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.model_selection&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">train_test_split&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.metrics&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">accuracy_score&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Split the data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">X_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_test&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">train_test_split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stratify&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">random_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">99&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Instantiate the model&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dt&lt;/span>&lt;span class="o">=&lt;/span> &lt;span class="n">DecisionTreeClassifier&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_depth&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">random_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">99&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Fit the model to the training data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Predict from the test set&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y_pred&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_test&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Evaluate the accuracy&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">accuracy_score&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_pred&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You can plot decision tree regions like in the image above. The code below assumes you already have a &amp;ldquo;logreg&amp;rdquo; model and a &amp;ldquo;dt&amp;rdquo; model.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Make a list of your models&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">models&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">logreg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dt&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Show the decision regions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plot_labeled_decision_regions&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">models&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Classification Metrics and the Confusion Matrix</title><link>https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/</guid><description>&lt;h1 id="metrics-for-classification-models">Metrics for Classification Models
&lt;/h1>&lt;p>The word &amp;ldquo;acccuracy&amp;rdquo; is often used broadly to cover how close to the truth something is, but in data science the meaning is more specific. There are also metrics other than accuracy that will judge how correct a model is, and the one you choose depends on the purpose of your study. It might be preferable to have a false positive, or it might be more preferable to avoid a false negative, for example.&lt;/p>
&lt;h2 id="negativespositives-in-classification">Negatives/Positives in Classification
&lt;/h2>&lt;p>There are four potential outcomes for your predictions when performing binary classification:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>True Positive (TP)&lt;/strong>: You correctly labeled a positive data point as positive.&lt;/li>
&lt;li>&lt;strong>False Positive (FP)&lt;/strong>: You incorrectly labeled a negative data point as positive.&lt;/li>
&lt;li>&lt;strong>True Negative (TN)&lt;/strong>: You correctly labeled a negative data point as negative.&lt;/li>
&lt;li>&lt;strong>False Negative (FN)&lt;/strong>: You incorrectly labeled a positive data point as negative.&lt;/li>
&lt;/ul>
&lt;p>As I mentioned, you will have different concerns based on the purpose of your analysis. For example, I did a machine learning assignment in school where I was using features to predict which students might need additional academic support. In my case, I was not overly concerned with false positives - if the school started academic intervention for a student who ended up not needing it, it would not be harmful for the student and the staff leading the support group would soon notice.&lt;/p>
&lt;p>In other cases, you might be more concerned about false positives. For example, a spam detector that flags important emails as spam would not be useful to a client.&lt;/p>
&lt;p>A &lt;strong>confusion matrix&lt;/strong> is a two-by-two grid which displays all of the values for TP, TN, FP, and FN.&lt;/p>
&lt;p>&lt;img src="https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/2025-01-22_confusion-matrix.png"
width="549"
height="337"
srcset="https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/2025-01-22_confusion-matrix_hu_6532b076fcc4abed.png 480w, https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/2025-01-22_confusion-matrix_hu_1741056d0a0490f.png 1024w"
loading="lazy"
alt="Confusion matrix"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>
&lt;a class="link" href="https://rumn.medium.com/precision-recall-and-f1-explained-with-10-ml-use-case-6ef2fbe458e5" target="_blank" rel="noopener"
>Source&lt;/a>&lt;/p>
&lt;h2 id="metrics">Metrics
&lt;/h2>&lt;p>These four classification outcomes lead to four metrics for assessing a classification model:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Accuracy&lt;/strong>: Out of all predictions, how many were correct?
&lt;ul>
&lt;li>Formula: (TP + TN) / (TP + TN + FP + FN)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Precision&lt;/strong>: Out of all the times the model predicted a positive, how often was that positive correct?
&lt;ul>
&lt;li>Formula: TP / (TP + FP)&lt;/li>
&lt;li>High precision = lower FP rate&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Recall&lt;/strong>: Out of all the data points that should have been predicted as positive, how many were accurately predicted?
&lt;ul>
&lt;li>Formula: TP / (TP + FN)&lt;/li>
&lt;li>High recall = lower FN rate&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>F1 Score&lt;/strong>: The harmonic mean of precision and recall, giving them equal weight. Useful when you have imbalanced data or want to account for both FP and FN.
&lt;ul>
&lt;li>Formula: 2 ((Precision * Recall) / (Precision + Recall))&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Relying on the wrong metric can lead to a false understanding of how well your model is performing. For example, a model might have a high accuracy score, but this might simply be the result of imbalanced data.&lt;/p>
&lt;p>Let&amp;rsquo;s say you are trying to classify if something is a fish or a cow. If your data is 99% fish, and your model predicts that every sincle datapoint is a fish, then the accuracy score would be 99%. It sounds good, but it does not actually mean your model is good at differentiating between fish and cows.&lt;/p>
&lt;h2 id="python-applications">Python Applications
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Import the libraries&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.metrics&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">classification_report&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">confusion_matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Instantiate the classifier&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">knn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">KNeighborsClassifier&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_neighbors&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">7&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Split the data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">X_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_test&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">train_test_split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">random_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">99&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Fit the model with the training data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">knn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Predict the labels for the test set&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y_pred&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">knn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_test&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Generate a confusion matrix from the predicted labels and test labels&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">confusion_matrix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_pred&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Generate metrics for the predicted and test labels&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">classification_report&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_pred&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Sample results of confusion matrix and classification report:&lt;/p>
&lt;p>&lt;img src="https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/2025-01-22_code-output.png"
width="465"
height="214"
srcset="https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/2025-01-22_code-output_hu_f78032a8a2d7f0f.png 480w, https://embergen.github.io/p/classification-metrics-and-the-confusion-matrix/2025-01-22_code-output_hu_cf5c90d566b59141.png 1024w"
loading="lazy"
alt="code output"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="521px"
>&lt;/p></description></item></channel></rss>